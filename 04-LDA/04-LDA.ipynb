{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# stematisation : \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "\n",
    "# bags of modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# accurracy \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# gridsearch \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: atterlep@vela.acs.oakland.edu (Cardinal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: miner@kuhub.cc.ukans.edu\\nSubject: Re: A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: atterlep@vela.acs.oakland.edu (Cardinal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: vzhivov@superior.carleton.ca (Vladimir Z...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...\n",
       "1  From: atterlep@vela.acs.oakland.edu (Cardinal ...\n",
       "2  From: miner@kuhub.cc.ukans.edu\\nSubject: Re: A...\n",
       "3  From: atterlep@vela.acs.oakland.edu (Cardinal ...\n",
       "4  From: vzhivov@superior.carleton.ca (Vladimir Z..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data', sep=\",\", header=None)\n",
    "\n",
    "data.columns = ['text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a collection of emails that are not labelled. Let's try extract topics from them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ You're used to it by now... Clean up! Store the cleaned text in a new dataframe column \"clean_text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\AppData\\Local\\Temp\\ipykernel_15580\\4134947805.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace(punctuation, \"\").str.lower().str.replace(r'\\b\\d+\\b', '')\n",
      "C:\\Users\\utilisateur\\AppData\\Local\\Temp\\ipykernel_15580\\4134947805.py:4: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data['text'] = data['text'].str.replace(punctuation, \"\").str.lower().str.replace(r'\\b\\d+\\b', '')\n"
     ]
    }
   ],
   "source": [
    "#  Remove punctuation\n",
    "string.punctuation\n",
    "for punctuation in string.punctuation : \n",
    "    data['text'] = data['text'].str.replace(punctuation, \"\").str.lower().str.replace(r'\\b\\d+\\b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from gldcunixbcccolumbiaedu gary l dare\\nsubje...</td>\n",
       "      <td>gldcunixbcccolumbiaedu gary l dare subject sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from atterlepvelaacsoaklandedu cardinal ximene...</td>\n",
       "      <td>atterlepvelaacsoaklandedu cardinal ximenez sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from minerkuhubccukansedu\\nsubject re ancient ...</td>\n",
       "      <td>minerkuhubccukansedu subject ancient books org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from atterlepvelaacsoaklandedu cardinal ximene...</td>\n",
       "      <td>atterlepvelaacsoaklandedu cardinal ximenez sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from vzhivovsuperiorcarletonca vladimir zhivov...</td>\n",
       "      <td>vzhivovsuperiorcarletonca vladimir zhivov subj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>from jerrybeskimocom jerry kaufman\\nsubject re...</td>\n",
       "      <td>jerrybeskimocom jerry kaufman subject prayers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>from golchowyalchemychemutorontoca gerald olch...</td>\n",
       "      <td>golchowyalchemychemutorontoca gerald olchowy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>from jaynemmaltguildorg jayne kulikauskas\\nsub...</td>\n",
       "      <td>jaynemmaltguildorg jayne kulikauskas subject q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>from sclarkepasutorontoca susan clark\\nsubject...</td>\n",
       "      <td>sclarkepasutorontoca susan clark subject picks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>from lmvecwestminsteracuk william hargreaves\\n...</td>\n",
       "      <td>lmvecwestminsteracuk william hargreaves subjec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1199 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     from gldcunixbcccolumbiaedu gary l dare\\nsubje...   \n",
       "1     from atterlepvelaacsoaklandedu cardinal ximene...   \n",
       "2     from minerkuhubccukansedu\\nsubject re ancient ...   \n",
       "3     from atterlepvelaacsoaklandedu cardinal ximene...   \n",
       "4     from vzhivovsuperiorcarletonca vladimir zhivov...   \n",
       "...                                                 ...   \n",
       "1194  from jerrybeskimocom jerry kaufman\\nsubject re...   \n",
       "1195  from golchowyalchemychemutorontoca gerald olch...   \n",
       "1196  from jaynemmaltguildorg jayne kulikauskas\\nsub...   \n",
       "1197  from sclarkepasutorontoca susan clark\\nsubject...   \n",
       "1198  from lmvecwestminsteracuk william hargreaves\\n...   \n",
       "\n",
       "                                             clean_text  \n",
       "0     gldcunixbcccolumbiaedu gary l dare subject sta...  \n",
       "1     atterlepvelaacsoaklandedu cardinal ximenez sub...  \n",
       "2     minerkuhubccukansedu subject ancient books org...  \n",
       "3     atterlepvelaacsoaklandedu cardinal ximenez sub...  \n",
       "4     vzhivovsuperiorcarletonca vladimir zhivov subj...  \n",
       "...                                                 ...  \n",
       "1194  jerrybeskimocom jerry kaufman subject prayers ...  \n",
       "1195  golchowyalchemychemutorontoca gerald olchowy s...  \n",
       "1196  jaynemmaltguildorg jayne kulikauskas subject q...  \n",
       "1197  sclarkepasutorontoca susan clark subject picks...  \n",
       "1198  lmvecwestminsteracuk william hargreaves subjec...  \n",
       "\n",
       "[1199 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(clean_text) :\n",
    "    \"\"\"nltk le fait automatiquement : il prend la string et la sÃ©pare en mots\"\"\"\n",
    "    return word_tokenize(clean_text)\n",
    "\n",
    "def remov(clean_text): \n",
    "    \"\"\"prends ma liste de stopwords\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "# conserve les mots si ils ne sont pas dans la liste du dessus\n",
    "    words = [w for w in clean_text if w not in stop_words]\n",
    "# prend une liste de string et la remets en une liste avec tous les mots \n",
    "    words = ' '.join(words)\n",
    "    return words\n",
    "\n",
    "# remove stopwords\n",
    "data['clean_text']= data['text'].apply(tokenize)\n",
    "data['clean_text'] = data['clean_text'].apply(remov)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Train an LDA model to extract potential topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(data['clean_text'])\n",
    "data_vectorized = vectorizer.transform(data['clean_text'])\n",
    "lda_model = LatentDirichletAllocation(n_components = 2).fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize potential topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ The function to print the words associated with the potential topics is already made for you. You just have to pass the correct arguments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "[('bos', 3.0924912738025427), ('chi', 2.797943810494836), ('pit', 2.4077360253351867), ('det', 2.38142219862452), ('tor', 1.8893588144294724), ('buf', 1.8576178810419401), ('cal', 1.8531954285690289), ('que', 1.634100104859753), ('stl', 1.6267623388971522), ('nyi', 1.5834232077713832)]\n",
      "Topic 1\n",
      "[('god', 29.912972885875064), ('would', 25.80717670634672), ('one', 23.016584105516607), ('subject', 22.420375456587685), ('organization', 21.537672320806617), ('university', 21.473175880618513), ('lines', 21.46427500086195), ('writes', 20.397380678360175), ('people', 20.371659123305967), ('game', 19.557774220862203)]\n"
     ]
    }
   ],
   "source": [
    "def print_topics(model, vectorizer): \n",
    "    for idx, topic in enumerate (model.components_):\n",
    "        print(\"Topic %d\" %(idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "            for i in topic.argsort()[:-10 -1 : -1]])\n",
    "\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : 0.049565961302878346\n",
      "topic 1 : 0.9504340386971216\n"
     ]
    }
   ],
   "source": [
    "clean_text_vectorized = vectorizer.transform(data['clean_text'])\n",
    "lda_vector = lda_model.transform(clean_text_vectorized)\n",
    "\n",
    "print(\"topic 0 :\", lda_vector[0][0])\n",
    "print(\"topic 1 :\", lda_vector[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50398095, 0.50371152, 0.50481415, 0.5060452 , 0.50393756,\n",
       "       0.50180376, 0.50637134, 0.50206863, 0.50297531, 0.503775  ,\n",
       "       0.5031373 , 0.50084097, 0.50247568, 0.50443294, 0.50025716,\n",
       "       0.50122351, 0.50092514, 0.50317737, 0.50342399, 0.50087219])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer.get_params\n",
    "# impact des 20 permiers tokens de la premiÃ¨re matrice \n",
    "lda_model.components_[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict topic of new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ You can now use your LDA model to predict the topic of a new text. First, use your vectorizer to vectorize the example. Then, use your LDA model to predict the topic of the vectorized example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= [\"\"\"Il y a des natures purement contemplatives et tout Ã  fait impropres Ã  lâ€™action, \n",
    "qui cependant, sous une impulsion mystÃ©rieuse et inconnue, agissent quelquefois avec \n",
    "une rapiditÃ© dont elles se seraient crues elles-mÃªmes incapables.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21866227 0.78133773]]\n"
     ]
    }
   ],
   "source": [
    "vec = vectorizer.transform(text)\n",
    "print(lda_model.transform(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le text semble appartenir Ã  la catÃ©gorie 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
